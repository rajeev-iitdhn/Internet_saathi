{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"age and gender.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1SODF0kEcxK-wpjWbkkJXU78ejdyW5aef","authorship_tag":"ABX9TyNQBDrSlkKADctfrNPPcBEN"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"LKrYG6YEbUj-","colab_type":"code","outputId":"99b18961-3c7b-47e7-da71-c42a4f75fe3b","executionInfo":{"status":"ok","timestamp":1588573978200,"user_tz":-330,"elapsed":14080,"user":{"displayName":"Rajeev Ranjan Srivastwa","photoUrl":"","userId":"14210186617697197545"}},"colab":{"base_uri":"https://localhost:8080/","height":420}},"source":["pip install cvlib"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting cvlib\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/56/f57aa012b3fb8b22f46cc9016a7198d7571b82d21c8a257dfca8d387c99b/cvlib-0.2.5.tar.gz (10.0MB)\n","\u001b[K     |████████████████████████████████| 10.1MB 60kB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from cvlib) (1.18.3)\n","Collecting progressbar\n","  Downloading https://files.pythonhosted.org/packages/a3/a6/b8e451f6cff1c99b4747a2f7235aa904d2d49e8e1464e0b798272aa84358/progressbar-2.5.tar.gz\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from cvlib) (2.23.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from cvlib) (7.0.0)\n","Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (from cvlib) (2.4.1)\n","Requirement already satisfied: imutils in /usr/local/lib/python3.6/dist-packages (from cvlib) (0.5.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->cvlib) (2020.4.5.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->cvlib) (2.9)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->cvlib) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->cvlib) (3.0.4)\n","Building wheels for collected packages: cvlib, progressbar\n","  Building wheel for cvlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for cvlib: filename=cvlib-0.2.5-cp36-none-any.whl size=10044204 sha256=7542d9638dfbf464cfa08546942eba9a89b5f2ce9693055f28afda0fd38dfcf2\n","  Stored in directory: /root/.cache/pip/wheels/6a/cb/43/ba188c823836640d8f22ee1f6ff792a0c83a8b66eabf52b219\n","  Building wheel for progressbar (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for progressbar: filename=progressbar-2.5-cp36-none-any.whl size=12074 sha256=a97ceb5c9bd6b1a81a06650c0c51ee954648f7cdcbf190d1734c3c6ff1f50076\n","  Stored in directory: /root/.cache/pip/wheels/c0/e9/6b/ea01090205e285175842339aa3b491adeb4015206cda272ff0\n","Successfully built cvlib progressbar\n","Installing collected packages: progressbar, cvlib\n","Successfully installed cvlib-0.2.5 progressbar-2.5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oLzKPPzQb9Cd","colab_type":"code","colab":{}},"source":["import os\n","os.chdir(\"/content/drive/My Drive/ML_works/Internet_sathi/IS_ML_PROBLEMS/profile_verification\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UiMPYE7WbIRb","colab_type":"code","outputId":"3d8674be-6d00-450c-9075-da03279b3ee0","executionInfo":{"status":"ok","timestamp":1588574096703,"user_tz":-330,"elapsed":3476,"user":{"displayName":"Rajeev Ranjan Srivastwa","photoUrl":"","userId":"14210186617697197545"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from keras.preprocessing.image import img_to_array\n","from keras.models import load_model\n","from keras.utils import get_file\n","import numpy as np\n","import argparse\n","import cv2\n","import os\n","import cvlib as cv"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"38OK3KRNbJ1a","colab_type":"code","colab":{}},"source":["# ap = argparse.ArgumentParser()\n","# ap.add_argument(\"-i\", \"--image\", required=True,\thelp=\"path to input image\")\n","# args = ap.parse_args()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8QEUj93Ubk7e","colab_type":"code","colab":{}},"source":["dwnld_link = \"https://github.com/arunponnusamy/cvlib/releases/download/v0.2.0/gender_detection.model\"\n","model_path = get_file(\"gender_detection.model\", dwnld_link,\n","                     cache_subdir=\"pre-trained\", cache_dir=os.getcwd())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yp_3j9LMd7vi","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AVksVEe5cPky","colab_type":"code","colab":{}},"source":["# import the necessary packages\n","import numpy as np\n","import urllib\n","import cv2\n","import urllib.request\n","from urllib.request import urlopen\n","# METHOD #1: OpenCV, NumPy, and urllib\n","def url_to_image(url):\n","\t# download the image, convert it to a NumPy array, and then read\n","\t# it into OpenCV format\n","\tresp = urlopen(url)\n","\timage = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n","\timage = cv2.imdecode(image, cv2.IMREAD_COLOR)\n","\t# return the image\n","\treturn image"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1n3Xg84_lDyl","colab_type":"code","colab":{}},"source":["model = load_model(model_path)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FWDI7QS0dmTh","colab_type":"code","colab":{}},"source":["image=url_to_image(\"https://storage.googleapis.com/internetsaathi-prod/c9dd54b5-83b5-4164-9d70-c9fd1f0609fd.jpg\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HCNGOcFIxtEY","colab_type":"code","colab":{}},"source":["image=cv2.imread(\"Screenshot from 2020-04-23 12-41-19.png\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dGfoqcIRfQTJ","colab_type":"code","outputId":"4e4600d0-3c98-4f6b-9748-b85117d110f4","executionInfo":{"status":"ok","timestamp":1587963193895,"user_tz":-330,"elapsed":717,"user":{"displayName":"Rajeev Ranjan Srivastwa","photoUrl":"","userId":"14210186617697197545"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["if image is None:\n","    print(\"Could not read input image\")\n","    exit()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Could not read input image\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Fb-Ni5ihfa60","colab_type":"code","outputId":"7e8d97f1-e4cc-4d7b-ac10-76a6671963c2","executionInfo":{"status":"error","timestamp":1587986349023,"user_tz":-330,"elapsed":1355,"user":{"displayName":"Rajeev Ranjan Srivastwa","photoUrl":"","userId":"14210186617697197545"}},"colab":{"base_uri":"https://localhost:8080/","height":229}},"source":["image=cv2.imread(\"Screenshot from 2020-04-23 12-19-31.png\")\n","face, confidence = cv.detect_face(image)\n","classes = ['man','woman']\n","\n","lev=[]\n","confid=[]\n","for idx, f in enumerate(face):\n","\n","     # get corner points of face rectangle       \n","    (startX, startY) = f[0], f[1]\n","    (endX, endY) = f[2], f[3]\n","    (delx,dely)=f[2]-f[0],f[3]-f[1]\n","\n","    cv2.rectangle(image, (startX,startY), (endX,endY), (0,255,0), 2)\n","    face_crop = np.copy(image[startY:endY,startX:endX])\n","    face_crop = cv2.resize(face_crop, (96,96))\n","    face_crop = face_crop.astype(\"float\") / 255.0\n","    face_crop = img_to_array(face_crop)\n","    face_crop = np.expand_dims(face_crop, axis=0)\n","    conf = model.predict(face_crop)[0]\n","    idx = np.argmax(conf)\n","    label = classes[idx]\n","    lev.append(label)\n","    confid.append(conf[idx]*100)\n","print(lev)"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-62-b9bb03bb198a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Screenshot from 2020-04-23 12-19-31.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mface\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfidence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_face\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'man'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'woman'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'cv' is not defined"]}]},{"cell_type":"code","metadata":{"id":"N96lUZUy1nfo","colab_type":"code","outputId":"47674e1a-b1be-4315-a6d2-4c6c282348aa","executionInfo":{"status":"error","timestamp":1587962302315,"user_tz":-330,"elapsed":873,"user":{"displayName":"Rajeev Ranjan Srivastwa","photoUrl":"","userId":"14210186617697197545"}},"colab":{"base_uri":"https://localhost:8080/","height":162}},"source":["face"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-a4ed216a5cbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mface\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'face' is not defined"]}]},{"cell_type":"code","metadata":{"id":"qnuvf3oTfgd3","colab_type":"code","colab":{}},"source":["# lev=[]\n","# confid=[]\n","# for idx, f in enumerate(face):\n","\n","#      # get corner points of face rectangle       \n","#     (startX, startY) = f[0], f[1]\n","#     (endX, endY) = f[2], f[3]\n","\n","#     cv2.rectangle(image, (startX,startY), (endX,endY), (0,255,0), 2)\n","#     face_crop = np.copy(image[startY:endY,startX:endX])\n","#     face_crop = cv2.resize(face_crop, (96,96))\n","#     face_crop = face_crop.astype(\"float\") / 255.0\n","#     face_crop = img_to_array(face_crop)\n","#     face_crop = np.expand_dims(face_crop, axis=0)\n","\n","#     conf = model.predict(face_crop)[0]\n","#     # print(conf)\n","#     # print(classes)\n","\n","#     idx = np.argmax(conf)\n","#     label = classes[idx]\n","#     lev.append(label)\n","#     confid.append(conf[idx]*100)\n","\n","#     label = \"{}: {:.2f}%\".format(label, conf[idx] * 100)\n","#     # lev.append(label)\n","\n","#     Y = startY - 10 if startY - 10 > 10 else startY + 10\n","\n","#     # write label and confidence above face rectangle\n","#     cv2.putText(image, label, (startX, Y),  cv2.FONT_HERSHEY_SIMPLEX,\n","#                 0.7, (0, 255, 0), 2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hvu7i98pzpBw","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"grciAtL4zo_C","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"H-gno_ex7hCT","colab_type":"code","colab":{}},"source":["import os"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cLBYKtD5_kSQ","colab_type":"code","colab":{}},"source":["os.chdir(\"/content/drive/My Drive/ML_works/Internet_sathi/IS_ML_PROBLEMS/profile_verification/age_and_gender\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"35-17mm3rtZI","colab_type":"code","colab":{}},"source":["# import cv2\n","# import math\n","# import argparse\n","\n","# def highlightFace(net, frame, conf_threshold=0.7):\n","#     frameOpencvDnn=frame.copy()\n","#     frameHeight=frameOpencvDnn.shape[0]\n","#     frameWidth=frameOpencvDnn.shape[1]\n","#     blob=cv2.dnn.blobFromImage(frameOpencvDnn, 1.0, (300, 300), [104, 117, 123], True, False)\n","\n","#     net.setInput(blob)\n","#     detections=net.forward()\n","#     faceBoxes=[]\n","#     for i in range(detections.shape[2]):\n","#         confidence=detections[0,0,i,2]\n","#         if confidence>conf_threshold:\n","#             x1=int(detections[0,0,i,3]*frameWidth)\n","#             y1=int(detections[0,0,i,4]*frameHeight)\n","#             x2=int(detections[0,0,i,5]*frameWidth)\n","#             y2=int(detections[0,0,i,6]*frameHeight)\n","#             faceBoxes.append([x1,y1,x2,y2])\n","#             cv2.rectangle(frameOpencvDnn, (x1,y1), (x2,y2), (0,255,0), int(round(frameHeight/150)), 8)\n","#     return frameOpencvDnn,faceBoxes\n","\n","\n","# parser=argparse.ArgumentParser()\n","# parser.add_argument('--image')\n","\n","# args=parser.parse_args()\n","\n","# faceProto=\"opencv_face_detector.pbtxt\"\n","# faceModel=\"opencv_face_detector_uint8.pb\"\n","# ageProto=\"age_deploy.prototxt\"\n","# ageModel=\"age_net.caffemodel\"\n","# genderProto=\"gender_deploy.prototxt\"\n","# genderModel=\"gender_net.caffemodel\"\n","\n","# MODEL_MEAN_VALUES=(78.4263377603, 87.7689143744, 114.895847746)\n","# ageList=['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']\n","# genderList=['Male','Female']\n","\n","# faceNet=cv2.dnn.readNet(faceModel,faceProto)\n","# ageNet=cv2.dnn.readNet(ageModel,ageProto)\n","# genderNet=cv2.dnn.readNet(genderModel,genderProto)\n","\n","# video=cv2.VideoCapture(args.image if args.image else 0)\n","# padding=20\n","# while cv2.waitKey(1)<0:\n","#     hasFrame,frame=video.read()\n","#     if not hasFrame:\n","#         cv2.waitKey()\n","#         break\n","\n","#     resultImg,faceBoxes=highlightFace(faceNet,frame)\n","#     if not faceBoxes:\n","#         print(\"No face detected\")\n","\n","#     for faceBox in faceBoxes:\n","#         face=frame[max(0,faceBox[1]-padding):\n","#                    min(faceBox[3]+padding,frame.shape[0]-1),max(0,faceBox[0]-padding)\n","#                    :min(faceBox[2]+padding, frame.shape[1]-1)]\n","\n","#         blob=cv2.dnn.blobFromImage(face, 1.0, (227,227), MODEL_MEAN_VALUES, swapRB=False)\n","#         genderNet.setInput(blob)\n","#         genderPreds=genderNet.forward()\n","#         gender=genderList[genderPreds[0].argmax()]\n","#         print(f'Gender: {gender}')\n","\n","#         ageNet.setInput(blob)\n","#         agePreds=ageNet.forward()\n","#         age=ageList[agePreds[0].argmax()]\n","#         print(f'Age: {age[1:-1]} years')\n","\n","#         cv2.putText(resultImg, f'{gender}, {age}', (faceBox[0], faceBox[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,255,255), 2, cv2.LINE_AA)\n","#         cv2.imshow(\"Detecting age and gender\", resultImg)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ttK1khZor_rC","colab_type":"code","colab":{}},"source":["def saathi_profile_verification(input_image):\n","  #saathi \n","  import os\n","  os.chdir(\"/content/drive/My Drive/ML_works/Internet_sathi/IS_ML_PROBLEMS/profile_verification/age_and_gender\")\n","  import cv2\n","  import math\n","  # import argparse\n","  import pandas as pd\n","  # img=cv2.imread(\"Screenshot from 2020-04-23 12-27-07.png\")\n","  img=cv2.imread(\"{x}\".format(x=str(input_image)))\n","  def highlightFace(net, frame, conf_threshold=0.7):\n","    frameOpencvDnn=frame.copy()\n","    frameHeight=frameOpencvDnn.shape[0]\n","    frameWidth=frameOpencvDnn.shape[1]\n","    blob=cv2.dnn.blobFromImage(frameOpencvDnn, 1.0, (300, 300), [104, 117, 123], True, False)\n","\n","    net.setInput(blob)\n","    detections=net.forward()\n","    faceBoxes=[]\n","    for i in range(detections.shape[2]):\n","        confidence=detections[0,0,i,2]\n","        if confidence>conf_threshold:\n","            x1=int(detections[0,0,i,3]*frameWidth)\n","            y1=int(detections[0,0,i,4]*frameHeight)\n","            x2=int(detections[0,0,i,5]*frameWidth)\n","            y2=int(detections[0,0,i,6]*frameHeight)\n","            faceBoxes.append([x1,y1,x2,y2])\n","            cv2.rectangle(frameOpencvDnn, (x1,y1), (x2,y2), (0,255,0), int(round(frameHeight/150)), 8)\n","    return frameOpencvDnn,faceBoxes\n","  faceProto=\"opencv_face_detector.pbtxt\"\n","  faceModel=\"opencv_face_detector_uint8.pb\"\n","  ageProto=\"age_deploy.prototxt\"\n","  ageModel=\"age_net.caffemodel\"\n","  genderProto=\"gender_deploy.prototxt\"\n","  genderModel=\"gender_net.caffemodel\"\n","\n","  MODEL_MEAN_VALUES=(78.4263377603, 87.7689143744, 114.895847746)\n","  ageList=['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']\n","  genderList=['Male','Female']\n","\n","  faceNet=cv2.dnn.readNet(faceModel,faceProto)\n","  ageNet=cv2.dnn.readNet(ageModel,ageProto)\n","  genderNet=cv2.dnn.readNet(genderModel,genderProto)\n","\n","  resultImg,faceBoxes=highlightFace(faceNet,img)\n","  padding=20\n","  gender_of_person=[]\n","  age_of_person=[]\n","  if not faceBoxes:\n","    output=\"No face detected\"\n","  for faceBox in faceBoxes:\n","    if len(list(faceBoxes))!=1:\n","      output=\"please upload a single non edited image.\" \n","      break\n","    if (img.shape[0]<400) & (img.shape[1]<400):\n","      output=\"please upload a good quality image\"\n","      break\n","    face=img[max(0,faceBox[1]-padding):\n","                min(faceBox[3]+padding,img.shape[0]-1),max(0,faceBox[0]-padding)\n","                :min(faceBox[2]+padding, img.shape[1]-1)]\n","    blob=cv2.dnn.blobFromImage(face, 1.0, (227,227), MODEL_MEAN_VALUES, swapRB=False)\n","    genderNet.setInput(blob)\n","    genderPreds=genderNet.forward()\n","    gender=genderList[genderPreds[0].argmax()]\n","    # print(f'Gender: {gender}')\n","    gender_of_person.append(gender)\n","    ageNet.setInput(blob)\n","    agePreds=ageNet.forward()\n","    age=ageList[agePreds[0].argmax()]\n","    age_of_person.append(age)\n","    # print(f'Age: {age[1:-1]} years')\n","    # if len(list(faceBoxes))==1:\n","    output=\"Age of the person is in range{x} and gender is {y}\".format(x=age_of_person[0], y=gender_of_person[0])\n","  return output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fC4lCISrMTpN","colab_type":"code","outputId":"1214394c-75de-4054-a51f-fd12dc58ba07","executionInfo":{"status":"ok","timestamp":1588581974944,"user_tz":-330,"elapsed":1275,"user":{"displayName":"Rajeev Ranjan Srivastwa","photoUrl":"","userId":"14210186617697197545"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["saathi_profile_verification(\"mohit.png\")"],"execution_count":53,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Age of the person is in range(25-32) and gender is Male'"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"code","metadata":{"id":"IJfmmX5nWenY","colab_type":"code","colab":{}},"source":["def beneficiary_profile_verification(input_image):\n","  # Beneficiary\n","  import os\n","  os.chdir(\"/content/drive/My Drive/ML_works/Internet_sathi/IS_ML_PROBLEMS/profile_verification/age_and_gender\")\n","  import cv2\n","  import math\n","  import argparse\n","  import pandas as pd\n","\n","  img=cv2.imread(\"{x}\".format(x=str(input_image)))\n","\n","  def highlightFace(net, frame, conf_threshold=0.7):\n","      frameOpencvDnn=frame.copy()\n","      frameHeight=frameOpencvDnn.shape[0]\n","      frameWidth=frameOpencvDnn.shape[1]\n","      blob=cv2.dnn.blobFromImage(frameOpencvDnn, 1.0, (300, 300), [104, 117, 123], True, False)\n","\n","      net.setInput(blob)\n","      detections=net.forward()\n","      faceBoxes=[]\n","      for i in range(detections.shape[2]):\n","          confidence=detections[0,0,i,2]\n","          if confidence>conf_threshold:\n","              x1=int(detections[0,0,i,3]*frameWidth)\n","              y1=int(detections[0,0,i,4]*frameHeight)\n","              x2=int(detections[0,0,i,5]*frameWidth)\n","              y2=int(detections[0,0,i,6]*frameHeight)\n","              faceBoxes.append([x1,y1,x2,y2])\n","              cv2.rectangle(frameOpencvDnn, (x1,y1), (x2,y2), (0,255,0), int(round(frameHeight/150)), 8)\n","      return frameOpencvDnn,faceBoxes\n","\n","  faceProto=\"opencv_face_detector.pbtxt\"\n","  faceModel=\"opencv_face_detector_uint8.pb\"\n","  ageProto=\"age_deploy.prototxt\"\n","  ageModel=\"age_net.caffemodel\"\n","\n","  MODEL_MEAN_VALUES=(78.4263377603, 87.7689143744, 114.895847746)\n","  ageList=['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']\n","\n","  faceNet=cv2.dnn.readNet(faceModel,faceProto)\n","  ageNet=cv2.dnn.readNet(ageModel,ageProto)\n","\n","  resultImg,faceBoxes=highlightFace(faceNet,img)\n","  padding=20\n","  age_of_person=[]\n","  if not faceBoxes:\n","    output=\"No face detected\"\n","  for faceBox in faceBoxes:\n","    if len(list(faceBoxes))!=1:\n","      output=\"please upload a single non edited image.\"\n","      break\n","    if img.shape[0]<400 & img.shape[1]<400:\n","      output=\"please upload a good quality image\"\n","      break\n","    face=img[max(0,faceBox[1]-padding):\n","                min(faceBox[3]+padding,img.shape[0]-1),max(0,faceBox[0]-padding)\n","                :min(faceBox[2]+padding, img.shape[1]-1)]\n","    blob=cv2.dnn.blobFromImage(face, 1.0, (227,227), MODEL_MEAN_VALUES, swapRB=False)\n","    ageNet.setInput(blob)\n","    agePreds=ageNet.forward()\n","    age=ageList[agePreds[0].argmax()]\n","    age_of_person.append(age)\n","  # if len(list(faceBoxes))==1:\n","    output=\"Age of the person is in range{x}.\".format(x=age_of_person[0])\n","  return output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2DiI7TFGM967","colab_type":"code","outputId":"0eb6b3ef-f3df-4833-ca02-95fc82a85b06","executionInfo":{"status":"ok","timestamp":1588582019419,"user_tz":-330,"elapsed":1100,"user":{"displayName":"Rajeev Ranjan Srivastwa","photoUrl":"","userId":"14210186617697197545"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["beneficiary_profile_verification(\"mohit.png\")"],"execution_count":55,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Age of the person is in range(25-32).'"]},"metadata":{"tags":[]},"execution_count":55}]},{"cell_type":"code","metadata":{"id":"cdlWQC5SNFkQ","colab_type":"code","outputId":"f0c66292-2dc4-40a1-c063-842163603a88","executionInfo":{"status":"ok","timestamp":1588574194604,"user_tz":-330,"elapsed":1263,"user":{"displayName":"Rajeev Ranjan Srivastwa","photoUrl":"","userId":"14210186617697197545"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["ssaathi_profile_verification(\"mohit.png\")"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Age of the person is in range(25-32) and gender is Male'"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"i5FoC-k54mjk","colab_type":"code","colab":{}},"source":["img=cv2.imread(\"Screenshot from 2020-04-23 12-41-19.png\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ksoTsPDI52tN","colab_type":"code","outputId":"236b26f1-bd6e-4f92-bf45-647674a266ba","executionInfo":{"status":"error","timestamp":1587962343800,"user_tz":-330,"elapsed":846,"user":{"displayName":"Rajeev Ranjan Srivastwa","photoUrl":"","userId":"14210186617697197545"}},"colab":{"base_uri":"https://localhost:8080/","height":346}},"source":["resultImg,faceBoxes=highlightFace(faceNet,img)\n","padding=20\n","gender_of_person=[]\n","age_of_person=[]\n","if not faceBoxes:\n","    print(\"No face detected\")\n","for faceBox in faceBoxes:\n","    face=img[max(0,faceBox[1]-padding):\n","                min(faceBox[3]+padding,img.shape[0]-1),max(0,faceBox[0]-padding)\n","                :min(faceBox[2]+padding, img.shape[1]-1)]\n","    blob=cv2.dnn.blobFromImage(face, 1.0, (227,227), MODEL_MEAN_VALUES, swapRB=False)\n","    genderNet.setInput(blob)\n","    genderPreds=genderNet.forward()\n","    gender=genderList[genderPreds[0].argmax()]\n","    # print(f'Gender: {gender}')\n","    gender_of_person.append(gender)\n","    # print(gender)\n","    ageNet.setInput(blob)\n","    agePreds=ageNet.forward()\n","    age=ageList[agePreds[0].argmax()]\n","    age_of_person.append(age)\n","    # print(f'Age: {age[1:-1]} years')"],"execution_count":0,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-a1f2e929e1ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresultImg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfaceBoxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhighlightFace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfaceNet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgender_of_person\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mage_of_person\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfaceBoxes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-40df56999fa3>\u001b[0m in \u001b[0;36mhighlightFace\u001b[0;34m(net, frame, conf_threshold)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhighlightFace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mframeOpencvDnn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mframeHeight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframeOpencvDnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mframeWidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframeOpencvDnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'copy'"]}]},{"cell_type":"code","metadata":{"id":"RQ7MJCK9w4NC","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}